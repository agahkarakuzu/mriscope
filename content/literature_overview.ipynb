{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6f3bcb9-881e-4a85-ae69-8817979c347e",
   "metadata": {},
   "source": [
    "## The Landscape of Reproducible Neuroimaging Studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e725ef-025a-41cb-81aa-5adeb19dee2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "from thefuzz import fuzz\n",
    "import json\n",
    "import time \n",
    "import numpy as np\n",
    "import umap\n",
    "import os\n",
    "import pandas as pd\n",
    "import umap.plot\n",
    "import plotly.graph_objects as go\n",
    "from plotly.offline import plot\n",
    "from IPython.display import display, HTML\n",
    "import base64 \n",
    "\n",
    "# REQUIRED CELL\n",
    "\n",
    "DATA_ROOT = \"../data/repro_mri_scoping\"\n",
    "\n",
    "def get_by_id(paper_id):\n",
    "\n",
    "    response = requests.post(\n",
    "    'https://api.semanticscholar.org/graph/v1/paper/batch',\n",
    "    params={'fields': 'abstract,tldr,year,embedding'},\n",
    "    json={\"ids\": [paper_id]})\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def get_id(title):\n",
    "    \"\"\"\n",
    "    Query Semantic Scholar API by title.\n",
    "    \"\"\"\n",
    "    api_url = \"https://api.semanticscholar.org/graph/v1/paper/search\"\n",
    "    params = {\"query\": title}\n",
    "    \n",
    "    response = requests.get(api_url, params=params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        print(result['total'])\n",
    "        for re in result['data']:\n",
    "            print(re)\n",
    "            if fuzz.ratio(re['title'],title) > 90:\n",
    "                return re['paperId']\n",
    "            else:\n",
    "                return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def bulk_search(query,save_json):\n",
    "    \"\"\"\n",
    "    The returns 1000 results per query. If the total number of \n",
    "    hits is larger, the request should be iterated using tokens.\n",
    "    \"\"\"\n",
    "    query = \"(code | data | open-source | github | jupyter ) + (('MRI' + 'brain') | (MRI + 'neuroimaging')) + reproducib~\"\n",
    "    fields = \"abstract\"\n",
    "    url = f\"http://api.semanticscholar.org/graph/v1/paper/search/bulk?query={query}&fields={fields}\"\n",
    "    r = requests.get(url).json()\n",
    "    print(f\"Found {r['total']} documents\")\n",
    "    retrieved = 0\n",
    "    \n",
    "    with open(save_json, \"a\") as file:\n",
    "        while True:\n",
    "            if \"data\" in r:\n",
    "                retrieved += len(r[\"data\"])\n",
    "                print(f\"Retrieved {retrieved} papers...\")\n",
    "                for paper in r[\"data\"]:\n",
    "                    print(json.dumps(paper), file=file)\n",
    "            if \"token\" not in r:\n",
    "                break\n",
    "            r = requests.get(f\"{url}&token={r['token']}\").json()\n",
    "    print(f\"Retrieved {retrieved} papers. DONE\")\n",
    "\n",
    "def read_json_file(file_name):\n",
    "    with open(file_name, 'r') as json_file:\n",
    "        json_list = list(json_file)\n",
    "    return json_list\n",
    "\n",
    "def write_json_file(file_name, dict_content):\n",
    "    with open(file_name, 'w') as json_file:\n",
    "        json_file.write(json.dumps(dict_content))\n",
    "    \n",
    "def get_output_dir(file_name):\n",
    "    op_dir = \"../output\"\n",
    "    if not os.path.exists(op_dir):\n",
    "        os.mkdir(op_dir)\n",
    "    return os.path.join(op_dir,file_name)\n",
    "\n",
    "def flatten_dict(input):\n",
    "    result_dict = {}\n",
    "    # Iterate over the list of dictionaries\n",
    "    for cur_dict in input:\n",
    "        # Iterate over key-value pairs in each dictionary\n",
    "        for key, value in cur_dict.items():\n",
    "            # If the key is not in the result dictionary, create a new list\n",
    "            if key not in result_dict:\n",
    "                result_dict[key] = []\n",
    "            # Append the value to the list for the current key\n",
    "            result_dict[key].append(value)\n",
    "    return result_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe5d6ce-da48-4349-9029-665536767fc5",
   "metadata": {},
   "source": [
    "## Literature search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dc3918-3097-4977-ae91-8f88625ced0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPTIONAL CELL\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "literature_records = get_output_dir(\"literature_records.json\")\n",
    "search_terms = \"(code | data | open-source | github | jupyter ) + (('MRI' + 'brain') | (MRI + 'neuroimaging')) + reproducib~\"\n",
    "\n",
    "# This will save output/literature_records.json\n",
    "bulk_search(search_terms,literature_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511e20eb-6b8c-40dd-b2a9-303f6725ca18",
   "metadata": {},
   "source": [
    "## Add articles associated with the reproducibility insights\n",
    "\n",
    "To include the paper IDs of the articles highlighted by reproducibility insights, we will utilize filenames from the `repro_mri_scoping/repro_insights_parsed_nov23` directory. For more information about this dataset, please refer to the following details.\n",
    "\n",
    "````{admonition} Crawling content\n",
    "\n",
    "In order to reduce crawler activity on the target website, spider commands are not included in the notebook. Instead, the content obtained by using the crawler (November 2023) was provided as part of the dataset (`repro_mri_scoping/repro_insights_parsed_nov23`). \n",
    "\n",
    "You can run the following commands **in a terminal** to re-run the crawler:   \n",
    "\n",
    "```\n",
    "cd crawler  \n",
    "scrapy crawl mrmh\n",
    "```\n",
    "\n",
    "This will save the outputs in `crawler/crawler_outputs` directory. Files are named according to the captured URLs. To expand or narrow down the search interval, please modify the following block in `crawler/insights/spiders/mrmh.py`:\n",
    "\n",
    "```python\n",
    "                 for year in range(0, 4)  # assuming 2023\n",
    "                 for month in range(1, 13)\n",
    "                 for day in range(1, 32)]\n",
    "```\n",
    "\n",
    "After the crawling process is completed, the details of the article upon which the interview is based will be retrieved from the Semantic Scholar API. Following this, `title`, `abstract`, and `TLDR` (if availabe) sections will be prepended to each document. To perform these tasks on the crawled content, please run the following in terminal: \n",
    "\n",
    "```\n",
    "python parse_crawled_content.py\n",
    "```\n",
    "\n",
    "Parsed outputs will be saved to `crawler/crawler_output/collection`, filenames will attain `paperId` (the uniqe ID assigned by Semantic Scholar). \n",
    "\n",
    "```{note}\n",
    "Some of the content fetched from the web needs minor edits. Please see `crawler/insights/spiders/NOTES.txt`. \n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c2031e-67a8-4792-bb9e-2b7d3269dad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REQUIRED CELL\n",
    "\n",
    "# To load THE ORIGINAL LIST, please comment in the following\n",
    "lit_list = read_json_file(os.path.join(DATA_ROOT,\"literature_records.json\"))\n",
    "\n",
    "# Read the LATEST literature records returned by the above search\n",
    "# Note that this may include new results (i.e., new articles) \n",
    "\n",
    "#literature_records = get_output_dir(\"literature_records.json\")\n",
    "#lit_list = read_json_file(literature_records)\n",
    "\n",
    "# Collect all the paper IDs from the literature search\n",
    "lit_ids = [json.loads(entry)['paperId'] for entry in lit_list]\n",
    "\n",
    "# Get all paper ids for the articles linked to \n",
    "insights_path = os.path.join(DATA_ROOT,\"repro_insights_parsed_nov23\")\n",
    "insights_ids = [f.split(\".\")[0] for f in os.listdir(insights_path) if f.endswith('.txt')]\n",
    "\n",
    "# Combine all IDs (unique)\n",
    "paper_ids_all = list(set(lit_ids + insights_ids))\n",
    "print(f\"Total: {len(paper_ids_all)} papers \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba4bc68-cd1f-4df4-b1d6-f0ec2f42ad24",
   "metadata": {},
   "source": [
    "## Get details of the articles\n",
    "\n",
    "The following cell is commented out as it involves a series of API calls. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c675522-5664-4275-99da-61c15e26891f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL CELL \n",
    "\n",
    "# slices = [(0, 499), (499, 998), (998, None)]\n",
    "# request_fields = 'title,venue,year,embedding,citationCount'\n",
    "\n",
    "# results = []\n",
    "# for start, end in slices:\n",
    "#     print(len(paper_ids_all[start:end]))\n",
    "#     re = requests.post(\n",
    "#             'https://api.semanticscholar.org/graph/v1/paper/batch',\n",
    "#             params={'fields': request_fields},\n",
    "#             json={\"ids\": paper_ids_all[start:end]})\n",
    "#     if re.status_code == 200:\n",
    "#         print(f\"Got results {start}:{end} interval\")\n",
    "#         results.append(re.json())\n",
    "#         time.sleep(15) # Rate limiting.\n",
    "#     else:\n",
    "#         print(f\"WARNING slice {start}:{end} did not return results: {re.text}\")\n",
    "\n",
    "# ALTERNATIVE \n",
    "# The above API call should work fast as the requests are sent in batch. \n",
    "# However, it frequently throws 429 error. If that's the case, following will \n",
    "# also work, but takes much longer and a few articles may not be captured.\n",
    "\n",
    "# results = []\n",
    "# for cur_id in paper_ids_all:\n",
    "#     #print(len(paper_ids_all[start:end]))\n",
    "#     re = requests.get(\n",
    "#             f'https://api.semanticscholar.org/graph/v1/paper/{cur_id}',\n",
    "#             params={'fields': request_fields})\n",
    "#     if re.status_code == 200:\n",
    "#         results.append(re.json())\n",
    "#     else:\n",
    "#         print(f\"WARNING request for {cur_id} could not return results: {re.text}\")\n",
    "\n",
    "# # Write outputs\n",
    "# write_json_file(get_output_dir(\"literature_data.json\"),results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f671c90-8f1d-4a8d-b4d2-7e13ae94b508",
   "metadata": {},
   "source": [
    "## Gathering Articles with Embeddings\n",
    "\n",
    "While utilizing the Semantic Scholar API, it's important to note that not all returned article details include word embeddings generated by [SPECTER](https://github.com/allenai/scidocs) (v1) based on the title and abstract. Currently, we are filtering out articles where embeddings are available, totaling 612 out of 1098. Future efforts may focus on addressing missing data by running the SPECTER model on articles without embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21635063-839b-4a3e-a8de-1e74759602eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REQUIRED CELL\n",
    "\n",
    "# Load the ORIGINAL data\n",
    "lit_data = json.loads(read_json_file(os.path.join(DATA_ROOT,\"literature_data.json\"))[0])\n",
    "\n",
    "# If you'd like to read from the output directory (LATEST)\n",
    "#lit_data = json.loads(read_json_file(get_output_dir(\"literature_data.json\"))[0])\n",
    "\n",
    "papers_data = []\n",
    "\n",
    "for res in lit_data:\n",
    "    if 'embedding' in res.keys():\n",
    "        if res['embedding']:\n",
    "            cur_rec = {\"embedding\":res['embedding']['vector'],\n",
    "                       \"title\":res['title'],\n",
    "                       \"venue\": res['venue'],\n",
    "                       \"year\": res['year'],\n",
    "                       \"is_mrmh\": \"Other\",\n",
    "                       \"paperId\": res['paperId'],\n",
    "                       \"n_citation\": res['citationCount']}\n",
    "            if res['paperId'] in insights_ids:\n",
    "                cur_rec['is_mrmh'] = \"Highlights\"\n",
    "    \n",
    "            papers_data.append(cur_rec)\n",
    "\n",
    "papers_data_dict = papers_data\n",
    "# From a list of dicts to a dict of lists.\n",
    "papers_data = flatten_dict(papers_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb167780-e070-452e-9925-03ae27d82221",
   "metadata": {},
   "source": [
    "## Dimension reduction and visualization\n",
    "\n",
    "To visualize the semantic relationship between the captured articles (a 612x768 matrix), we are going to use Uniform Manifold Approximation and Projection (UMAP) method. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2eb721f-a0b1-45e7-b896-7d6f9c3a7300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REQUIRED CELL\n",
    "\n",
    "# Reduce to 2D feature\n",
    "umap_model_2d = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2,random_state=42)\n",
    "umap_2d = umap_model_2d.fit_transform(np.array(papers_data['embedding']))\n",
    "umap_2d_mapper = umap_model_2d.fit(np.array(papers_data['embedding']))\n",
    "\n",
    "# Reduce to 3D feature\n",
    "umap_model_3d = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=3,random_state=42)\n",
    "umap_3d = umap_model_3d.fit_transform(np.array(papers_data['embedding']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3beea4bf-e161-448f-a9e2-b677141ea74c",
   "metadata": {},
   "source": [
    "## Connectivity plot\n",
    "\n",
    "Create the connectivity plot to be used as a background image for the interactive data visualization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c5a1f5-21ad-42de-bcd3-24f191aea25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap.plot.connectivity(umap_2d_mapper, edge_bundling='hammer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7dc9c9-05ad-4283-bf2d-be4a6a3631e7",
   "metadata": {},
   "source": [
    "## Interactive plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e1bffd-697b-4851-8774-5134beead227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Plotly figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Scatter plot for UMAP in 2D\n",
    "scatter_2d = go.Scatter(\n",
    "    x=umap_2d[:, 0],\n",
    "    y=umap_2d[:, 1],\n",
    "    mode='markers',\n",
    "    marker = dict(color =[\"#562135\" if item == 'Highlights' else \"#f8aabe\" for item in papers_data['is_mrmh']],\n",
    "                                          size=9, \n",
    "                                          line= dict(color=\"#ff8080\",width=1),\n",
    "                                          opacity=0.9),\n",
    "    customdata= [f\"<b>{dat['title']}</b> <br>{dat['venue']} <br>Cited by: {dat['n_citation']} <br>{dat['year']}\" for dat in papers_data_dict],\n",
    "    hovertemplate='%{customdata}',\n",
    "    visible = True,\n",
    "    name='2D'\n",
    ")\n",
    "fig.add_trace(scatter_2d)\n",
    "\n",
    "# Add dropdown\n",
    "fig.update_layout(\n",
    "    updatemenus=[\n",
    "        dict(\n",
    "            type = \"buttons\",\n",
    "            direction = \"left\",\n",
    "            buttons=list([\n",
    "                dict(\n",
    "                    args=[{\"showscale\":True,\"marker\": dict(color =[\"#562135\" if item == 'Highlights' else \"#f8aabe\" for item in papers_data['is_mrmh']],\n",
    "                                          size=9, \n",
    "                                          line= dict(color=\"#ff8080\",width=1),\n",
    "                                          opacity=0.9)}],\n",
    "                    label=\"Highlights\",\n",
    "                    method=\"restyle\"\n",
    "                ),\n",
    "                dict(\n",
    "                    args=[{\"marker\": dict(color = np.log(papers_data['n_citation']),colorscale='Plotly3',size=9, colorbar=dict(thickness=10,title = \"Citation (log)\",tickvals= [0,max(papers_data['n_citation'])]))}],\n",
    "                    label=\"Citation\",\n",
    "                    method=\"restyle\"\n",
    "                ),\n",
    "                dict(\n",
    "                    args=[{\"marker\": dict(color = papers_data['year'],colorscale='Viridis',size=9,colorbar=dict(thickness=10, title=\"Year\"))}],\n",
    "                    label=\"Year\",\n",
    "                    method=\"restyle\"\n",
    "                )\n",
    "            ]),\n",
    "            pad={\"r\": 10, \"t\": 10},\n",
    "            showactive=True,\n",
    "            x=0.11,\n",
    "            xanchor=\"left\",\n",
    "            y=0.98,\n",
    "            yanchor=\"top\"\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "plotly_logo = base64.b64encode(open(os.path.join(DATA_ROOT,'sakurabg.png'), 'rb').read())\n",
    "\n",
    "fig.update_layout(plot_bgcolor='white',\n",
    "                    images= [dict(\n",
    "                    source='data:image/png;base64,{}'.format(plotly_logo.decode()),\n",
    "                    xref=\"paper\", yref=\"paper\",\n",
    "                    x=0.033,\n",
    "                    y=0.956,\n",
    "                    sizex=0.943, sizey=1,\n",
    "                    xanchor=\"left\",\n",
    "                    yanchor=\"top\",\n",
    "                    #sizing=\"stretch\",\n",
    "                    layer=\"below\")])\n",
    "\n",
    "fig.update_layout(yaxis={'visible': False, 'showticklabels': False})\n",
    "fig.update_layout(xaxis={'visible': False, 'showticklabels': False})\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(title='Sentient Array of Knowledge Unraveling and Assessment (SAKURA)',\n",
    "                 height = 850,\n",
    "                 width = 884,\n",
    "                hovermode='closest')\n",
    "\n",
    "plot(fig, filename = 'sakura.html')\n",
    "display(HTML('sakura.html'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99fb8a7-624f-42af-9525-37220f239262",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "# Scatter plot for UMAP in 3D\n",
    "scatter_3d = go.Scatter3d(\n",
    "    x=umap_3d[:, 0],\n",
    "    y=umap_3d[:, 1],\n",
    "    z=umap_3d[:, 2],\n",
    "    mode='markers',\n",
    "    marker = dict(color =[\"#562135\" if item == 'Highlights' else \"#f8aabe\" for item in papers_data['is_mrmh']],\n",
    "                                      size=9, \n",
    "                                      line= dict(color=\"#ff8080\",width=1),\n",
    "                                      opacity=0.9),\n",
    "    customdata= [f\"<b>{dat['title']}</b> <br>{dat['venue']} <br>Cited by: {dat['n_citation']} <br>{dat['year']}\" for dat in papers_data_dict],\n",
    "    hovertemplate='%{customdata}',\n",
    "    visible = True,\n",
    "    name='3D'\n",
    ")\n",
    "\n",
    "fig.add_trace(scatter_3d)\n",
    "\n",
    "fig.update_layout(\n",
    "    updatemenus=[\n",
    "        dict(\n",
    "            type = \"buttons\",\n",
    "            direction = \"left\",\n",
    "            buttons=list([\n",
    "                dict(\n",
    "                    args=[{\"marker\": dict(color =[\"#562135\" if item == 'Highlights' else \"#f8aabe\" for item in papers_data['is_mrmh']],\n",
    "                                          size=9,\n",
    "                                          line= dict(color=\"#ff8080\",width=1),\n",
    "                                          opacity=0.9)}],\n",
    "                    label=\"Highlights\",\n",
    "                    method=\"restyle\"\n",
    "                ),\n",
    "                dict(\n",
    "                    args=[{\"marker\": dict(color = np.log(papers_data['n_citation']),colorscale='Plotly3',size=9,colorbar=dict(thickness=10,tickvals= [0,max(papers_data['n_citation'])],title=\"Citation\"))}],\n",
    "                    label=\"Citation\",\n",
    "                    method=\"restyle\"\n",
    "                ),\n",
    "                dict(\n",
    "                    args=[{\"marker\": dict(color = papers_data['year'],colorscale='Viridis',size=9,colorbar=dict(thickness=10, title=\"Year\"))}],\n",
    "                    label=\"Year\",\n",
    "                    method=\"restyle\"\n",
    "                )\n",
    "            ]),\n",
    "            pad={\"r\": 10, \"t\": 10},\n",
    "            showactive=True,\n",
    "            x=0.11,\n",
    "            xanchor=\"left\",\n",
    "            y=0.98,\n",
    "            yanchor=\"top\"\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(title='UMAP 3D',\n",
    "                 height = 900,\n",
    "                 width = 900,\n",
    "                 hovermode='closest',\n",
    "                 template='plotly_dark')\n",
    "\n",
    "\n",
    "plot(fig, filename = 'sakura3d.html')\n",
    "display(HTML('sakura3d.html'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
